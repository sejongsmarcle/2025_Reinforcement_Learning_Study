{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Q-learning 실습\n",
        "시작점에서 탐욕 정책으로 이동하며 경로 기록\n",
        "-> 터미널 도착 시 보상을 확정\n",
        "-> 그 보상을 점차 줄어가며(당장의 확실하고 가까운 보상을 중요하게 여기고, 미래의 불확실한 보상을 덜 중요하게 여김) 경로를 거꾸로 따라 Q값을 갱신\n",
        "-> 초기화 후 다음 에피소드 반복"
      ],
      "metadata": {
        "id": "XPj5aG16ukqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CV7qxv9o_Bc",
        "outputId": "bc160630-b0d0-444d-f4d9-39d09511c338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "latest Q-values ... \n",
            "\n",
            "{(0, 0): {'U': 0, 'D': 0, 'L': 0, 'R': 0.682}, (0, 1): {'U': 0, 'D': 0, 'L': 0, 'R': 0.787}, (0, 2): {'U': 0, 'D': 0, 'L': 0, 'R': 0.894}, (0, 3): {'U': 1, 'D': 1, 'L': 1, 'R': 1}, (1, 0): {'U': 0, 'D': 0, 'L': 0, 'R': 0.326}, (1, 1): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (1, 2): {'U': 0, 'D': 0, 'L': 0.356, 'R': -0.18}, (1, 3): {'U': -1, 'D': -1, 'L': -1, 'R': -1}, (2, 0): {'U': 0, 'D': 0, 'L': 0.071, 'R': -0.001}, (2, 1): {'U': 0, 'D': 0, 'L': 0.093, 'R': -0.005}, (2, 2): {'U': 0, 'D': 0, 'L': 0.112, 'R': -0.027}, (2, 3): {'U': 0, 'D': 0, 'L': 0, 'R': -0.157}}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "BOARD_ROWS = 3\n",
        "BOARD_COLS = 4\n",
        "WIN_STATE = (0, 3) #에이전트가 도달하면 +1 보상\n",
        "LOSE_STATE = (1, 3) #에이전트가 도달하면 -1 보상\n",
        "BLOCKED_STATE = (1, 1) #장애물 존재\n",
        "START = (2, 0) #에피소드가 시작되는 위치\n",
        "DETERMINISTIC = False #False이면 확률적으로 미끄러짐이 발생\n",
        "class State: #환경\n",
        "    def __init__(self, state = START): #에이전트의 현재 위치를 저장\n",
        "        self.state = state\n",
        "        self.isEnd = False #현재 상태 win/lose 여부\n",
        "        self.determine = DETERMINISTIC\n",
        "\n",
        "    def giveReward(self): #보상함수\n",
        "        if self.state == WIN_STATE:\n",
        "            return 1\n",
        "        elif self.state == LOSE_STATE:\n",
        "            return -1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def isEndFunc(self): #종료 상태 판별\n",
        "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
        "            self.isEnd = True\n",
        "\n",
        "    def _chooseActionProb(self, action): #확률적 행동 처리(미끄러짐)\n",
        "    #불확실성을 구현\n",
        "        if action == \"U\": #80% 위쪽, 10% 왼쪽, 10% 오른쪽\n",
        "            return np.random.choice([\"U\", \"L\", \"R\"], p = [0.8, 0.1, 0.1])\n",
        "        if action == \"D\":\n",
        "            return np.random.choice([\"D\", \"L\", \"R\"], p = [0.8, 0.1, 0.1])\n",
        "        if action == \"L\":\n",
        "            return np.random.choice([\"L\", \"U\", \"D\"], p = [0.8, 0.1, 0.1])\n",
        "        if action == \"R\":\n",
        "            return np.random.choice([\"R\", \"U\", \"D\"], p = [0.8, 0.1, 0.1])\n",
        "\n",
        "    def nxtPosition(self, action): #다음 위치 계산\n",
        "        if self.determine: #deterministic 모드일 때 해당 방향으로 이동\n",
        "            if action == \"U\":\n",
        "                nxtState = (self.state[0] - 1, self.state[1])\n",
        "            elif action == \"D\":\n",
        "                nxtState = (self.state[0] + 1, self.state[1])\n",
        "            elif action == \"L\":\n",
        "                nxtState = (self.state[0], self.state[1] - 1)\n",
        "            else:\n",
        "                nxtState = (self.state[0], self.state[1] + 1)\n",
        "            self.determine = False\n",
        "        else: #stochastic 모드일 때 방향이 확률적으로 변할 수 있음\n",
        "            action = self._chooseActionProb(action)\n",
        "            self.determine = True\n",
        "            nxtState = self.nxtPosition(action)\n",
        "\n",
        "        if (nxtState[0] >= 0) and (nxtState[0] <= 2): #경계 조건 확인\n",
        "            if (nxtState[1] >= 0) and (nxtState[1] <= 3): #보드 안에 있는지\n",
        "                if nxtState != BLOCKED_STATE: #장애물인지\n",
        "                    return nxtState #유효하지 않으면 제자리 유지\n",
        "        return self.state\n",
        "\n",
        "class Agent: #에이전트(Q-learning 학습자)\n",
        "\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = [\"U\", \"D\", \"L\", \"R\"]\n",
        "        self.State = State()\n",
        "        self.isEnd = self.State.isEnd\n",
        "        self.lr = 0.2 #학습률\n",
        "        self.decay_gamma = 0.9 #감소율\n",
        "\n",
        "        #Q-Table 초기화\n",
        "        self.Q_values = {}\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                self.Q_values[(i, j)] = {}\n",
        "                for a in self.actions:\n",
        "                    self.Q_values[(i, j)][a] = 0 #각 행동의 초기Q값을 0으로 세팅\n",
        "\n",
        "    def chooseAction(self): #행동 선택\n",
        "        max_nxt_reward = 0\n",
        "        action = \"\"\n",
        "\n",
        "        for a in self.actions:\n",
        "            current_position = self.State.state\n",
        "            nxt_reward = self.Q_values[current_position][a]\n",
        "            if nxt_reward >= max_nxt_reward:\n",
        "                action = a\n",
        "                max_nxt_reward = nxt_reward\n",
        "                #현재 상태에서 Q값이 가장 큰 행동을 선택\n",
        "        return action\n",
        "\n",
        "    def takeAction(self, action): #행동 수행 후 상태 이동\n",
        "        position = self.State.nxtPosition(action)\n",
        "        return State(state = position)\n",
        "        #선택한 행동을 환경에 전달 -> 새로운 state 반환\n",
        "\n",
        "    def reset(self): #에피소드 종료 후 초기화\n",
        "        self.states = []\n",
        "        self.State = State()\n",
        "        self.isEnd = self.State.isEnd\n",
        "\n",
        "    def play(self, episodes = 10): #학습 과정\n",
        "        i = 0\n",
        "        while i < episodes:\n",
        "            if self.State.isEnd:\n",
        "                reward = self.State.giveReward()\n",
        "                for a in self.actions:\n",
        "                    self.Q_values[self.State.state][a] = reward\n",
        "                    #현재 상태가 종료 상태하면 -> 보상을 해당 상태의 Q값에 반영\n",
        "\n",
        "                for s in reversed(self.states): #Q값 역전파 업데이트\n",
        "                    current_q_value = self.Q_values[s[0]][s[1]]\n",
        "                    reward = current_q_value + self.lr * (self.decay_gamma * reward - current_q_value)\n",
        "                    self.Q_values[s[0]][s[1]] = round(reward, 3)\n",
        "                    #s[0]은 state, s[1]은 action\n",
        "                self.reset()\n",
        "                i += 1 #다음 에피소드로 이동\n",
        "            else: #에피소드 진행 중\n",
        "                action = self.chooseAction() #종료가 아니라면 행동 선택\n",
        "                self.states.append([(self.State.state), action])\n",
        "                self.State = self.takeAction(action)\n",
        "                self.State.isEndFunc()\n",
        "                self.isEnd = self.State.isEnd\n",
        "\n",
        "ag = Agent()\n",
        "\n",
        "ag.play(1000) #에피소드 1000번 학습\n",
        "print(\"latest Q-values ... \\n\")\n",
        "print(ag.Q_values) #최종 Q-Table 출"
      ]
    }
  ]
}