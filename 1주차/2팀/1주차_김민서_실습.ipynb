def PlayEpisode(qMap, policy, returns):
    """
    1개 에피소드를 수행하고, 끝난 뒤 그 에피소드의 결과 보상으로
    방문한 (상태,행동)들에 대해 MC 평가/정책개선을 수행한다.

    - '탐험적 시작(Exploring Starts)': 초기 상태와 첫 행동을 무작위로 선택
      → 모든 (s,a)가 충분히 방문될 기회를 확보
    """
    # --- 탐험적 시작: 무작위 초기 상태 샘플 ---
    playerSum = np.random.randint(11, 22)     # 플레이어 합 11~21
    dealerOpenCard = np.random.randint(1, 11) # 딜러 오픈 카드 1(A)~10
    usableAce = bool(np.random.randint(0, 2)) # 에이스를 11로 쓸 수 있는 상태인지 (True/False)

    player = Player(playerSum, usableAce, dealerOpenCard)
    dealer = Dealer([dealerOpenCard])

    stateActionInfo = StateActionInfo()

    # --- 탐험적 시작: 무작위 '첫 행동' 선택 ---
    hitAction = bool(np.random.randint(0, 2))  # True=Hit, False=Stand
    stateActionInfo.AddPair((player.GetState(), hitAction))

    # --- 플레이어 턴(첫 행동 반영) ---
    if hitAction:
        # 첫 행동이 히트면 카드 1장 받음
        player.ReceiveCard(newCard())

        # 이후에는 버스트가 아니고, 현재 정책이 히트를 권하면 계속 히트
        while not player.Bust() and player.ShouldHit(policy):
            # 방문 (상태,Hit) 기록(첫방문만)
            stateActionInfo.AddPair((player.GetState(), True))
            # 카드 1장 더 받음
            player.ReceiveCard(newCard())

    # --- 플레이어 버스트 체크 ---
    if player.Bust():
        # 플레이어가 버스트면 보상 -1로 에피소드 종료
        EvaluateAndImprovePolicy(qMap, policy, returns,
                                 stateActionInfo.stateActionPairs, -1)
        return

    # --- 스탠드 확정 상태 기록 ---
    # (버스트가 아니라면 결국 스탠드 상태로 넘어가므로, 마지막 상태에 대해 (s,Stand)도 기록)
    stateActionInfo.AddPair((player.GetState(), False))

    # --- 딜러 턴 ---
    # 딜러는 카드 1장 받고(두 번째 카드), 합이 17 미만이면 계속 히트
    dealer.ReceiveCard(newCard())
    while not dealer.Bust() and dealer.ShouldHit():
        dealer.cards.append(newCard())

    # --- 최종 승패/보상 계산 ---
    # 플레이어 승: +1, 패: -1, 무: 0
    if dealer.Bust() or dealer.GetValue() < player.GetValue():
        EvaluateAndImprovePolicy(qMap, policy, returns,
                                 stateActionInfo.stateActionPairs, 1)
    elif dealer.GetValue() > player.GetValue():
        EvaluateAndImprovePolicy(qMap, policy, returns,
                                 stateActionInfo.stateActionPairs, -1)
    else:
        EvaluateAndImprovePolicy(qMap, policy, returns,
                                 stateActionInfo.stateActionPairs, 0)


# =========================
# 테이블/정책 초기화
# =========================
qMap = { }     # Q(s,a) 저장 딕셔너리: key = (state, actionBool)
policy = { }   # 정책 π(s): True=Hit, False=Stand
returns = { }  # 방문수 N(s,a): 증분 평균 계산용

# 상태 공간을 순회하며 Q/N/정책 초기값을 설정
for playerSum in range(11, 22):      # 플레이어 합 11~21
    for usableAce in range(2):       # 0(False), 1(True)
        for dealersCard in range(1, 11):  # 딜러 오픈 1(A)~10
            playerState = (playerSum, bool(usableAce), dealersCard)

            # Q 초기값 0, 방문수 0
            qMap[(playerState, False)] = 0
            qMap[(playerState, True)]  = 0
            returns[(playerState, False)] = 0
            returns[(playerState, True)]  = 0

            # 초기 정책: 20/21은 스탠드, 나머지는 히트
            if playerSum == 20 or playerSum == 21:
                policy[playerState] = False  # Stand
            else:
                policy[playerState] = True   # Hit

# =========================
# MC 학습(에피소드 반복)
# =========================
for i in range(100000):  # 충분히 많은 에피소드로 수렴 유도
    PlayEpisode(qMap, policy, returns)
