{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nA6zw3mRoj9j"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # 수학/배열 연산용\n",
        "\n",
        "BOARD_ROWS = 3      # 보드 행 개수\n",
        "BOARD_COLS = 4      # 보드 열 개수\n",
        "\n",
        "WIN_STATE = (0, 3)      # 승리 위치\n",
        "LOSE_STATE = (1, 3)     # 패배 위치\n",
        "BLOCKED_STATE = (1, 1)  # 이동 불가 칸\n",
        "START = (2, 0)          # 시작 위치\n",
        "\n",
        "DETERMINISTIC = False   # False면 행동 결과에 랜덤성 있음\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class State:\n",
        "    def __init__(self, state = START):\n",
        "        self.state = state           # 현재 위치\n",
        "        self.isEnd = False           # 게임 종료 여부\n",
        "        self.determine = DETERMINISTIC  # 결정론적 이동 여부\n",
        "\n",
        "    def giveReward(self):\n",
        "        # 현재 위치에 따른 보상 반환\n",
        "        if self.state == WIN_STATE:\n",
        "            return 1\n",
        "        elif self.state == LOSE_STATE:\n",
        "            return -1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def isEndFunc(self):\n",
        "        # 현재 위치가 승리/패배 상태면 종료 표시\n",
        "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
        "            self.isEnd = True\n",
        "\n",
        "    def _chooseActionProb(self, action):\n",
        "        # 확률적으로 행동 선택 (랜덤성)\n",
        "        if action == \"U\":\n",
        "            return np.random.choice([\"U\", \"L\", \"R\"], p=[0.8, 0.1, 0.1])\n",
        "        if action == \"D\":\n",
        "            return np.random.choice([\"D\", \"L\", \"R\"], p=[0.8, 0.1, 0.1])\n",
        "        if action == \"L\":\n",
        "            return np.random.choice([\"L\", \"U\", \"D\"], p=[0.8, 0.1, 0.1])\n",
        "        if action == \"R\":\n",
        "            return np.random.choice([\"R\", \"U\", \"D\"], p=[0.8, 0.1, 0.1])\n",
        "\n",
        "    def nxtPosition(self, action):\n",
        "        # 다음 위치 계산\n",
        "        if self.determine:\n",
        "            # 결정론적 이동\n",
        "            if action == \"U\":\n",
        "                nxtState = (self.state[0]-1, self.state[1])\n",
        "            elif action == \"D\":\n",
        "                nxtState = (self.state[0]+1, self.state[1])\n",
        "            elif action == \"L\":\n",
        "                nxtState = (self.state[0], self.state[1]-1)\n",
        "            else:\n",
        "                nxtState = (self.state[0], self.state[1]+1)\n",
        "            self.determine = False\n",
        "        else:\n",
        "            # 확률적 이동\n",
        "            action = self._chooseActionProb(action)\n",
        "            self.determine = True\n",
        "            nxtState = self.nxtPosition(action)\n",
        "\n",
        "        # 이동 범위 확인 및 막힌 칸 처리\n",
        "        if (0 <= nxtState[0] <= 2) and (0 <= nxtState[1] <= 3) and (nxtState != BLOCKED_STATE):\n",
        "            return nxtState\n",
        "        return self.state  # 이동 불가 시 현재 위치 유지\n"
      ],
      "metadata": {
        "id": "861l2KZipW5L"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.states = []               # 에이전트가 이동한 상태와 행동 기록\n",
        "        self.actions = [\"U\", \"D\", \"L\", \"R\"]  # 가능한 행동\n",
        "        self.State = State()            # 현재 상태(State 객체)\n",
        "        self.isEnd = self.State.isEnd   # 현재 상태가 종료인지 여부\n",
        "        self.lr = 0.2                   # 학습률(Learning Rate)\n",
        "        self.decay_gamma = 0.9          # 할인율(Gamma)\n",
        "\n",
        "        # Q-테이블 초기화: 모든 상태에서 모든 행동의 값 0으로 시작\n",
        "        self.Q_values = {}\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                self.Q_values[(i, j)] = {}\n",
        "                for a in self.actions:\n",
        "                    self.Q_values[(i, j)][a] = 0\n",
        "\n",
        "    def chooseAction(self):\n",
        "        # 현재 상태에서 Q값이 가장 높은 행동 선택 (탐욕적)\n",
        "        max_nxt_reward = 0\n",
        "        action = \"\"\n",
        "        for a in self.actions:\n",
        "            current_position = self.State.state\n",
        "            nxt_reward = self.Q_values[current_position][a]\n",
        "            if nxt_reward >= max_nxt_reward:\n",
        "                action = a\n",
        "                max_nxt_reward = nxt_reward\n",
        "        return action\n",
        "\n",
        "    def takeAction(self, action):\n",
        "        # 선택한 행동으로 이동 후 새로운 State 객체 반환\n",
        "        position = self.State.nxtPosition(action)\n",
        "        return State(state = position)\n",
        "\n",
        "    def reset(self):\n",
        "        # 에피소드 종료 후 초기화\n",
        "        self.states = []\n",
        "        self.State = State()\n",
        "        self.isEnd = self.State.isEnd\n",
        "\n",
        "    def play(self, episodes = 10):\n",
        "        # Q-learning 학습 루프\n",
        "        i = 0\n",
        "        while i < episodes:\n",
        "            if self.State.isEnd:\n",
        "                # 종료 상태면 보상 부여 및 Q값 업데이트\n",
        "                reward = self.State.giveReward()\n",
        "                for a in self.actions:\n",
        "                    self.Q_values[self.State.state][a] = reward\n",
        "                for s in reversed(self.states):\n",
        "                    current_q_value = self.Q_values[s[0]][s[1]]\n",
        "                    reward = current_q_value + self.lr * (self.decay_gamma * reward - current_q_value)\n",
        "                    self.Q_values[s[0]][s[1]] = round(reward, 3)\n",
        "                self.reset()\n",
        "                i += 1\n",
        "            else:\n",
        "                # 학습 중이면 행동 선택 후 이동\n",
        "                action = self.chooseAction()\n",
        "                self.states.append([(self.State.state), action])  # 상태-행동 기록\n",
        "                self.State = self.takeAction(action)\n",
        "                self.State.isEndFunc()  # 종료 여부 확인\n",
        "                self.isEnd = self.State.isEnd\n"
      ],
      "metadata": {
        "id": "5uZUo6HUqF-B"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ag = Agent()        # 에이전트 객체 생성 (Q-table 초기화 등)\n",
        "\n",
        "ag.play(1000)       # 1000 에피소드 동안 학습 실행\n",
        "                     # - 매 에피소드마다 상태-행동 쌍을 기록\n",
        "                     # - 종료 상태에서 보상 받고 Q-table 갱신\n",
        "\n",
        "print(\"latest Q-values ... \\n\")\n",
        "print(ag.Q_values)  # 학습이 끝난 후 모든 상태에서 행동별 Q값 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX5eZ3vBqZM_",
        "outputId": "8c2020fc-c8da-49fe-bea3-2521546083ad"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "latest Q-values ... \n",
            "\n",
            "{(0, 0): {'U': 0, 'D': 0, 'L': 0, 'R': 0.582}, (0, 1): {'U': 0, 'D': 0, 'L': 0, 'R': 0.729}, (0, 2): {'U': 0, 'D': 0, 'L': 0, 'R': 0.684}, (0, 3): {'U': 1, 'D': 1, 'L': 1, 'R': 1}, (1, 0): {'U': 0, 'D': 0, 'L': 0, 'R': 0.245}, (1, 1): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (1, 2): {'U': 0, 'D': 0, 'L': 0.311, 'R': -0.18}, (1, 3): {'U': -1, 'D': -1, 'L': -1, 'R': -1}, (2, 0): {'U': 0, 'D': 0, 'L': 0.099, 'R': -0.001}, (2, 1): {'U': 0, 'D': 0, 'L': 0.099, 'R': -0.006}, (2, 2): {'U': 0, 'D': 0, 'L': 0.1, 'R': -0.031}, (2, 3): {'U': 0, 'D': 0, 'L': 0, 'R': -0.18}}\n"
          ]
        }
      ]
    }
  ]
}
